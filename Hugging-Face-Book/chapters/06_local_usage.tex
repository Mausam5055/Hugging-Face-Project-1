\chapter{Using Hugging Face Locally}

The easiest way to use Hugging Face is through the \texttt{pipeline()} function. It handles all the complexity of pre-processing, model inference, and post-processing, wrapping it all into a single function call.

\section{Sentiment Analysis}
Let's begin with a classic NLP task: analyzing if a sentence is positive or negative.

\begin{lstlisting}[language=Python, caption=Sentiment Analysis with Pipeline]
from transformers import pipeline

# 1. Download the model automatically
# By default, this loads a DistilBERT model finetuned on sst-2
classifier = pipeline("sentiment-analysis")

# 2. Use it on a single sentence
result = classifier("I love learning about AI!")
print(result)
# Output: [{'label': 'POSITIVE', 'score': 0.999}]

# 3. Use it on a list of sentences (Process in batch)
results = classifier([
    "This works great!",
    "I am not happy with the service."
])
print(results)
# Output: [
#   {'label': 'POSITIVE', 'score': 0.99...},
#   {'label': 'NEGATIVE', 'score': 0.98...}
# ]
\end{lstlisting}

\section{Text Summarization}
Summarizing a long paragraph into a few sentences is a "Sequence-to-Sequence" task.

\begin{lstlisting}[language=Python, caption=Summarization]
summarizer = pipeline("summarization")

text = """
Hugging Face is a company that develops tools for building applications using machine learning. 
It is most notable for its transformers library built for natural language processing applications 
and its platform that allows users to share machine learning models and datasets.
"""

# max_length controls the output size in tokens
summary = summarizer(text, max_length=30, min_length=10, do_sample=False)
print(summary[0]['summary_text'])
\end{lstlisting}

\section{Question Answering}
The model can extract an answer from a given context.

\begin{lstlisting}[language=Python, caption=Question Answering]
qa_model = pipeline("question-answering")

context = "My name is Mausam and I live in Bangalore."
question = "Where do I live?"

answer = qa_model(question=question, context=context)
print(answer)
# Output: {'score': 0.98, 'start': 31, 'end': 40, 'answer': 'Bangalore'}
\end{lstlisting}

\section{Zero-Shot Classification}
This is magic. You can classify text into categories the model has \textit{never seen before} during training.

\begin{lstlisting}[language=Python, caption=Zero-Shot Classification]
classifier = pipeline("zero-shot-classification")

text = "Star Wars is a space opera franchise created by George Lucas."
candidate_labels = ["politics", "science", "movies"]

res = classifier(text, candidate_labels)
print(res)
# The model correctly assigns the highest score to "movies" 
# even specifically trained on this sentence.
\end{lstlisting}

\begin{tipbox}
The \texttt{pipeline} function automatically chooses a default model for the task. You can specify a specific model using the \texttt{model="model\_name"} argument.
\end{tipbox}
